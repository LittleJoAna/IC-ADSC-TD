3.Для чего необходимо знать асимптотическую сложность алгоритма
	3.1. Асимптотическая сложность алгоритма
	3.2. Оценка времени выполнения алгоритмов(Асимптотическая нотация)
		3.2.1. O Большое(O) - худшее время выполнения алгоритмов
		3.2.2. Тета Большое (Θ) - среднее время выполнения
		3.2.3. Омега Большое (Ω) - лучшее время выполнения алгоритмов
	3.3. Дополнительные материалы.

###########################(Методичка урока 2)
3.Для чего необходимо знать асимптотическую сложность алгоритма

Наверное, многие, читая эти строки, думают про себя, что они всю жизнь прекрасно
обходились без
знаний об алгоритмах, и, конечно же, в этих словах есть доля правды. Но если встанет
вопрос о
доказательстве эффективности или, наоборот, неэффективности какого-либо кода, то без
формального анализа уже не обойтись, а в серьезных проектах такая потребность возникает
регулярно.

3.1. Асимптотическая сложность алгоритма

Асимптотическая сложность (производительность) определяется функцией, которая указывает,
насколько ухудшается работа алгоритма с усложнением поставленной задачи. Такую функцию
записывают в круглых скобках, предваряя прописной буквой О.
Например, доступ к ячейке массива описывается как O(1), так как для доступа к ячейке
потребуется
всего одна элементарная операция, и сложность здесь не возрастает независимо от размера
массива.
O(N 2 ) означает, что, по мере увеличения количества входных данных, время работы
алгоритма
(использование памяти либо другой измеряемый параметр) возрастает квадратично. Если данных
станет вдвое больше, производительность алгоритма замедлится приблизительно в четыре раза.
При
увеличении количества входных данных в три раза, она станет меньше в девять раз.
Существуют пять основных правил для расчета асимптотической сложности алгоритма:
1. Если для математической функции f алгоритму необходимо выполнить определённые
действия f(N) раз, то для этого ему понадобится сделать O(f(N)) шагов.
© geekbrains.ru 2
2. Если алгоритм выполняет одну операцию, состоящую из O(f(N)) шагов, а затем вторую,
включающую O(g(N)) шагов, то общая производительность алгоритма для функций f и g
составит O(f(N) + g(N)).
3. Если алгоритму необходимо сделать O(f(N) + g(N)) шагов, и область значений N функции
f(N)
больше, чем у g(N), то асимптотическую сложность можно упростить до выражения O(f(N)).
4. Если алгоритму внутри каждого шага O(f(N)) одной операции приходится выполнять ещё
O(g(N)) шагов другой операции, то общая производительность алгоритма составит O(f(N) x
g(N)).
5. Постоянными множителями (константами) можно пренебречь. Если C является константой, то
O(C x f(N)) или O(f(C x N)) можно записать как O(f(N)).
Приведённые правила кажутся немного формальными из-за абстрактных функций f(N) и g(N), но
ими
очень легко пользоваться на практике. Ниже приведено несколько примеров, которые облегчат
понимание.

Правило 1
Если для математической функции f алгоритму необходимо выполнить определенные действия
f(N)
раз, то для этого ему понадобится сделать O(f(N)) шагов.
int findMax ( int size , int * array )
{
int result = array [ 0 ];
int i ;
for ( i = 1 ; i < size ; i ++)
if ( array [ i ] > result )
result = array [ i ];
return result ;
}
В качестве входного параметра программа использует массив целых чисел, результат
возвращается
в виде одного целого числа. В самом начале переменной max присваивается значение первого
элемента массива. Затем алгоритм перебирает оставшиеся элементы и сравнивает значение
каждого
из них с max. Если он находит большую величину, то приравнивает max к ней и по окончанию
цикла
возвращает наибольшее найденное значение. Алгоритм проверяет каждый из N элементов массива
всего один раз, поэтому его производительность составляет O(N).
Правило 2
Если алгоритм выполняет одну операцию, состоящую из O(f(N)) шагов, а затем вторую
операцию,
включающую O(g(N)) шагов, то общая производительность алгоритма для функций f и g составит
O(f(N) + g(N)).
Вернемся к алгоритму FindMax. На этот раз обратите внимание, что несколько строк в
действительности не включены в цикл. В следующей программе в комментариях справа приведён
порядок времени выполнения все тех же шагов.
© geekbrains.ru 3
int FindMax ( int size , int * array )
{
int result = array [ 0 ]; // O(1)
int i ;
for ( i = 1 ; i < size ; i ++) // O(N)
if ( array [ i ] > result )
result = array [ i ];
return result ; // O(1)
}
Итак, приведённый алгоритм выполняет один шаг отладки перед циклом и ещё один после него.
Каждый из них имеет производительность O(1) (это однократное действие), поэтому общее
время
работы алгоритма составит O(1 + N + 1). Если использовать обычную алгебру и преобразовать
выражение, то получится O(2 + N).
Правило 3
Если алгоритму необходимо сделать O(f(N) + g(N)) шагов, и область значений N функции f(N)
больше,
чем у g(N), то асимптотическую сложность можно упростить до выражения O(f(N)).
В предыдущем примере мы выяснили, что время работы алгоритма FindMax определяется
выражением O(2 + N). Если параметр N начнет возрастать, его значение превысит постоянную
величину 2, и предыдущее выражение можно будет упростить до O(N).
Игнорирование меньших функций позволяет пренебречь небольшими задачами отладки и очистки,
чтобы сосредоточить внимание на асимптотическом поведении алгоритма, которое
обнаруживается
при усложнении задачи. Другими словами, время, затраченное алгоритмом на построение
простых
структур данных перед выполнением объемного вычисления, является несущественным по
сравнению с длительностью основных расчётов.
Правило 4
Если алгоритму внутри каждого шага O(f(N)) одной операции приходится выполнять ещё O(g(N))
шагов другой операции, то общая производительность алгоритма составит O(f(N) x g(N)).
Рассмотрим алгоритм, который определяет, содержатся ли в массиве повторяющиеся элементы.
(Стоит отметить, что это не самый эффективный способ обнаружения дубликатов.)
int FindDublicates ( int N , int * array )
{
int i , j ;
for ( i = 0 ; i < N ; i ++)
for ( j = 0 ; j < N ; j ++)
if ( i != j )
if ( array [ i ] == array [ j ])
return 1 ;
// Если мы дошли до этого места, значит дубликатов нет
return 0 ;
}
© geekbrains.ru 4
Алгоритм содержит два цикла, один из которых является вложенным. Внешний цикл перебирает
все
элементы массива N, выполняя O(N) шагов. Внутри каждого такого шага внутренний цикл
повторно
пересматривает все N элементов массива, совершая те же O(N) шагов. Следовательно, общая
производительность алгоритма составит O(N x N) = O(N 2 ).
Правило 5
Постоянными множителями (константами) можно пренебречь. Если C является константой, то O(C
x
f(N)) или O(f(C x N)) можно записать как O(f(N)).
Снова посмотрите на алгоритм FindDuplicates из предыдущего примера и обратите внимание на
внутренний цикл, который представлен условием if. В рамках этого условия определяется,
равны ли
друг другу индексы i и j. Если нет, тогда сравниваются величины array[i] и array[j], в
случае их
совпадения возвращается значение true.
Пренебрегая дополнительным шагом в выражении return (как правило, он выполняется один
раз),
предположим, что срабатывают оба оператора if (а так и происходит в большинстве случаев),
тогда
внутренний цикл будет пройден за O(2N) шагов. Следовательно, общая производительность
алгоритма составит O(N x 2N) = O(2N 2 ). Последнее правило позволяет пренебречь
коэффициентом
2 и записать производительность алгоритма в виде O(N 2 ).
На самом деле, мы возвращаемся к сути асимптотической сложности: нужно выяснить, как
поведёт
себя алгоритм, если N начнёт возрастать. Предположим, вы увеличите N в два раза, то есть
будете
оперировать значением 2N. Теперь, если подставить фразу в выражение 2N^2, получится
следующее: 2 x (2N) 2 = 2 x 4N 2 = 8N 2 . Это и есть наша величина 2N 2 , только
умноженная на 4. Таким
образом, время работы алгоритма увеличится в четыре раза.
Теперь давайте оценим производительность алгоритма, используя упрощённое по правилу
выражение O(N 2 ). При подстановке в него 2N получим следующее: (2N) 2 = 4N 2 . То есть
наша
изначальная величина N 2 возросла в четыре раза, как и время работы алгоритма.
Из всего вышесказанного следует, что независимо от того, будете вы использовать
развёрнутую
формулу 2N 2 или ограничитесь просто N 2 , результат останется прежним: увеличение
сложности
задачи в два раза замедлит работу алгоритма в четыре раза. Таким образом, важной здесь
является
не константа 2, а тот факт, что время работы возрастает вместе с увеличением количества
вводов N 2 .
Важно понимать, что асимптотическая сложность даёт представление о теоретическом поведении
алгоритма. Практические результаты могут отличаться.

###########################(Из книги - Т.Кормен, Ч.Лейзерсон, Р.Ривест, К.Штайн -
Алгоритмы. Построение и анализ. - 2013.djvu. Страница 50)

3.2. Оценка времени выполнения алгоритмов(Асимптотическая нотация)
Анализируя алгоритм сортировки вставкой(Т.Кормен, Ч.Лейзерсон, Р.Ривест, К.Штайн -
Алгоритмы. Построение и анализ. - 2013. Страница 47), мы рассматривали как наилучший, так
и наихудший случай, когда элементы массива были рассортированы в порядке, обратном
требуемому. Далее в этой книге мы будем уделять основное внимание определению только
времени работы в наихудшем случае, т.е. максимального времени работы для любых входных
данных размером n. На то есть три причины:

3.2.1. O Большое(O) - худшее время выполнения алгоритмов
Время работы алгоритма в наихудшем случае - это верхник предел этой величины для любых
входных данных. Распологая этим значением, мы точно знаем, что для выполнения алгоритма
не потребуется большее количество времени. Не нужно будет делать каких-то сложных
предположений о времени работы и надеяться, что на самом деле эта величина не будет
превышена.

3.2.2. Тета Большое (Θ) - среднее время выполнения
Характер поведения "усреднённого" времени работы часто ничем не лучше поведения времени
работы для наихудшего случая. Предположим, что последовательность, к которой применяется
сортировка методом вставок, сформирована случайным образом. Сколько времени понадобится,
чтобы определить, в какое место массива A[1..j-1] следует поместить элемент A[j]? В
среднем половина элементов подмассива A[1..j-1] меньше,чем A[j], а половина больше этого
значения. таким образом, в среднем нужно проверить половину элементов подмассива
A[1..j-1], поэтому tj(t с нижней маленькой j - https://yadi.sk/i/Y7vlWKuqkTk_og)
приблизительно равно j/2. В результате получается, что среднее время работы алгоритма
является квадратичной функцией от количества входных элементов, т.е. характер этой
зависимости такой же, как и для времени работы в наихудшем случае.

3.2.3. Омега Большое (Ω) - лучшее время выполнения алгоритмов
В некоторых алгоритмах наихудший случай встречается достаточно часто. Например, если в
базе данных происходит посик информации, то наихудшему случаю соответствует ситуация,
когда нужная информация в базе данных отсутствует. В некоторых приложениях поиск
отсутствующей информации может происходить довольно часто.

3.3. Дополнительные материалы.
1. https://habr.com/post/188010/ - таблицы и графики позволяющие определить сложность
выполнения алгоритмов.
2. Т.Кормен, Ч.Лейзерсон, Р.Ривест, К.Штайн - Алгоритмы. Построение и анализ. - 2013
